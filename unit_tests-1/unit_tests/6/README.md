# Test bed for Phase 6 (Code Generation) of the the CS 460 Compiler.

Code generation is objectively the most difficult phase of the whole project. Now that the input files have been validated, through several passes of error handling, the compiler can generate an equivalent byte code that is capable of executing on a virtual environment. Thus, gone are the original evaluation metrics of only considering the terminal output of the reference compiler and a students compiler. We are now in need of new evaluation metrics; these metrics are as follows:
	1) Comparison of the original terminal output (just like phases 1 - 5).
	2) Comparison of the generated byte code for the students compiler and the reference compiler.
	3) Comparison of the executed byte code after it has been run on a virtual environment.

As such, the unit tests have been structurally changed to match the previously mentioned metrics and the directory structure is now as follows:
├── jasmin
│   ├── Espresso
│   ├── Espresso_Plus
│   └── Espresso_Star
├── log
│   └── Espresso
│       ├── BadTests
│       └── GoodTests
└── print
    ├── Espresso
    ├── Espresso_Plus
    └── Espresso_Star
				*) xyyzz_method_feature

					x := associated file {
						0 := AllocateAddress.java
						1 := GenerateCode.java
					}

					y := method_key {
						FOR_STAT
						NAME_EXPR,
						CONSTRUCTOR_DECL,
						etc ...
					}

					z := visitor feature 

Each parent directory (log, jasmin, and print) contain a set of unit test files (categorized by Espresso category) that implements behavior of a targeted visitor feature and is expected to be evaluated in one of 3 ways. 

1) log files
		These files will operate just like phases 1 - 5, where the output directed towards it terminal is the output to be compared.

2) jasmin files
		When you run your compiler a number of jasmin (.j) files will be created (ditto this for the reference compiler generating .rj files). 
		After your compiler generates the same number of .j as .rj files then you can concatenate all .j files together and .rj files together and compare them.
		The diff of the concatenated .j and .rj files must match for these tests to pass.

3) print files
		These tests files are the original test files and can be treated as jasmin files with an extra layer of checking. 
		After generating the required .j files you can convert each of them to java class files using the given jasmin shell script. 
		Once all requisite .class files are created they can be executed on a virtual machine (evm using the espresso shell script or jvm using the java command).
		It is the output of the virtual machine that must match for these tests to pass.

		There is a small caveat to these tests. How do you know what the output of the virtual machine is supposed to be so you can compare?
		At the head of each of these tests is a bulk comment that contains the expected output. 
		To compare the output of your code from the VM, you would need to do some string processing to remove the /* * */ comment fluff and then compare.
		That is a lot of complicated scripting work... which is why I have done it for you.

Accompanying these test files should be a file called manual_diff_6.sh.
That shell script will have all of the necessary code needed to filter the tests by category and evaluate the test file according to that category.

As a final note, everything stated in this README and more is covered by my grading guidelines handout.  

If either of those files is not included, ask the instructor or TA for it. If I'm no longer the TA for the course, then best of luck :)

_BZ Fall '24